# Flask Ollama Chat

A simple Flask web app for chatting with Llama 3.2 via Ollama.

## Setup
1. Install Ollama and pull the model: `ollama pull llama3.2:1b`
2. Install dependencies: `pip install flask requests`
3. Start Ollama: `ollama serve`
4. Run the app: `python app.py`
5. Open http://localhost:5000

## Features
- Simple chat interface
- Direct integration with local Ollama instance